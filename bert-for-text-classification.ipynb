{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:21.200007Z","iopub.execute_input":"2023-01-27T17:07:21.201637Z","iopub.status.idle":"2023-01-27T17:07:29.986406Z","shell.execute_reply.started":"2023-01-27T17:07:21.201577Z","shell.execute_reply":"2023-01-27T17:07:29.985412Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### note\nif you want to use other model, all you have to do is to import \n* Roberta : RobertaTokenizer , RobertaForSequenceClassification\n* Albert : AlbertTokenizer, AlbertForSequenceClassification\n* AutoModelForSequenceClassification : for more details check [hugging face documentation](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:29.987787Z","iopub.execute_input":"2023-01-27T17:07:29.988864Z","iopub.status.idle":"2023-01-27T17:07:30.108455Z","shell.execute_reply.started":"2023-01-27T17:07:29.988810Z","shell.execute_reply":"2023-01-27T17:07:30.107161Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'Tesla T4'"},"metadata":{}}]},{"cell_type":"code","source":"train_set = pd.read_csv(\"/kaggle/input/goodreads-books-reviews-290312/goodreads_train.csv\")\ntrain_set['rating'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:30.112058Z","iopub.execute_input":"2023-01-27T17:07:30.112563Z","iopub.status.idle":"2023-01-27T17:07:56.333299Z","shell.execute_reply.started":"2023-01-27T17:07:30.112534Z","shell.execute_reply":"2023-01-27T17:07:56.332347Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"4    313688\n5    265007\n3    188972\n2     72627\n0     30988\n1     28718\nName: rating, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Due to resource constraints, I wasn't able to use all the data. Consequently, I selected a subset from each class.","metadata":{}},{"cell_type":"code","source":"train_rate0 = train_set[train_set['rating']==0][:30987]\nprint(train_rate0.shape)\ntrain_rate1 = train_set[train_set['rating']==1][:28717]\nprint(train_rate1.shape)\ntrain_rate2 = train_set[train_set['rating']==2][:48000]\nprint(train_rate2.shape)\n\ntrain_rate3 = train_set[train_set['rating']==3][:48000]\nprint(train_rate3.shape)\n\ntrain_rate4 = train_set[train_set['rating']==4][:48000]\nprint(train_rate4.shape)\n\ntrain_rate5= train_set[train_set['rating']==5][:48000]\nprint(train_rate5.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:56.334694Z","iopub.execute_input":"2023-01-27T17:07:56.335173Z","iopub.status.idle":"2023-01-27T17:07:56.582734Z","shell.execute_reply.started":"2023-01-27T17:07:56.335135Z","shell.execute_reply":"2023-01-27T17:07:56.581605Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(30987, 11)\n(28717, 11)\n(48000, 11)\n(48000, 11)\n(48000, 11)\n(48000, 11)\n","output_type":"stream"}]},{"cell_type":"code","source":"new_train= pd.concat([train_rate0,train_rate1, train_rate2, train_rate3, train_rate4, train_rate5], axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:56.584402Z","iopub.execute_input":"2023-01-27T17:07:56.585117Z","iopub.status.idle":"2023-01-27T17:07:56.888185Z","shell.execute_reply.started":"2023-01-27T17:07:56.585077Z","shell.execute_reply":"2023-01-27T17:07:56.887186Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"new_train","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:56.889471Z","iopub.execute_input":"2023-01-27T17:07:56.889851Z","iopub.status.idle":"2023-01-27T17:07:56.915038Z","shell.execute_reply.started":"2023-01-27T17:07:56.889808Z","shell.execute_reply":"2023-01-27T17:07:56.913893Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                 user_id   book_id  \\\n3       8842281e1d1347389f2ab93d60773d4d  27161156   \n7       8842281e1d1347389f2ab93d60773d4d  24189224   \n13      8842281e1d1347389f2ab93d60773d4d  16158596   \n54      8842281e1d1347389f2ab93d60773d4d       151   \n58      8842281e1d1347389f2ab93d60773d4d    259028   \n...                                  ...       ...   \n164051  50c82fffc560a19ca82ab87ee3a95b92  30258320   \n164052  50c82fffc560a19ca82ab87ee3a95b92  18222716   \n164054  50c82fffc560a19ca82ab87ee3a95b92  22299763   \n164056  50c82fffc560a19ca82ab87ee3a95b92   2731276   \n164065  50c82fffc560a19ca82ab87ee3a95b92  17901125   \n\n                               review_id  rating  \\\n3       ced5675e55cd9d38a524743f5c40996e       0   \n7       dbc01e2438df7a87ee3dc16ee23a53e5       0   \n13      6ff8bbc4856aa403bbd8990407c9c77a       0   \n54      daab5f2752243787e471e2ac01bf12fc       0   \n58      fb4acc8a30bac6bf1414a03303d43c26       0   \n...                                  ...     ...   \n164051  092f3faaf9db761193c6726a8fc82bf8       5   \n164052  4237dbb90067669edbc7978df8f5a31c       5   \n164054  7208191258727d84a429cd83a1dc4910       5   \n164056  0f2b6e84833089d7507bb611aa1053be       5   \n164065  ba1f7345a172e60dcc24591e8debec58       5   \n\n                                              review_text  \\\n3       Recommended reading to understand what is goin...   \n7       Numerous people in publishing have told me thi...   \n13                            Recommended by David Risher   \n54      Well if Melanie says its her BBE, I gotta chec...   \n58               If steve recommends it, it must be good!   \n...                                                   ...   \n164051  Alternative history/fantasy. Very interesting,...   \n164052  ** spoiler alert ** \\n I greatly enjoyed this ...   \n164054  I dreamed this book--went to sleep and woke up...   \n164056  It's obvious that a great deal of care and tim...   \n164065  Love these characters and the setting. While t...   \n\n                            date_added                    date_updated  \\\n3       Wed Nov 09 17:37:04 -0800 2016  Wed Nov 09 17:38:20 -0800 2016   \n7       Fri May 29 17:48:57 -0700 2015  Fri May 29 17:49:40 -0700 2015   \n13      Mon Jul 07 10:56:15 -0700 2014  Mon Jul 07 10:56:39 -0700 2014   \n54      Mon May 14 12:55:56 -0700 2007  Sat Jan 07 11:40:38 -0800 2017   \n58      Thu Jan 18 11:09:48 -0800 2007  Mon Mar 09 00:38:30 -0700 2015   \n...                                ...                             ...   \n164051  Sat Apr 15 16:56:30 -0700 2017  Sat Apr 15 16:57:51 -0700 2017   \n164052  Thu Jan 19 17:33:23 -0800 2017  Thu Jan 19 17:35:58 -0800 2017   \n164054  Mon Dec 05 10:47:40 -0800 2016  Sat Apr 15 17:05:41 -0700 2017   \n164056  Wed Nov 23 16:43:49 -0800 2016  Wed Nov 23 16:45:01 -0800 2016   \n164065  Fri Jul 10 20:04:58 -0700 2015  Fri Jul 10 20:09:36 -0700 2015   \n\n                               read_at                      started_at  \\\n3                                  NaN                             NaN   \n7                                  NaN                             NaN   \n13                                 NaN                             NaN   \n54                                 NaN                             NaN   \n58                                 NaN                             NaN   \n...                                ...                             ...   \n164051                             NaN                             NaN   \n164052  Fri Jan 06 00:00:00 -0800 2017  Mon Jan 02 00:00:00 -0800 2017   \n164054                             NaN                             NaN   \n164056  Thu Sep 01 00:00:00 -0700 2016                             NaN   \n164065  Fri Jul 10 00:00:00 -0700 2015  Tue Jul 07 00:00:00 -0700 2015   \n\n        n_votes  n_comments  \n3             5           1  \n7            11           5  \n13            0           0  \n54            1           2  \n58            2           2  \n...         ...         ...  \n164051        0           0  \n164052        0           0  \n164054        0           0  \n164056        0           0  \n164065        0           0  \n\n[251704 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>rating</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>27161156</td>\n      <td>ced5675e55cd9d38a524743f5c40996e</td>\n      <td>0</td>\n      <td>Recommended reading to understand what is goin...</td>\n      <td>Wed Nov 09 17:37:04 -0800 2016</td>\n      <td>Wed Nov 09 17:38:20 -0800 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24189224</td>\n      <td>dbc01e2438df7a87ee3dc16ee23a53e5</td>\n      <td>0</td>\n      <td>Numerous people in publishing have told me thi...</td>\n      <td>Fri May 29 17:48:57 -0700 2015</td>\n      <td>Fri May 29 17:49:40 -0700 2015</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>16158596</td>\n      <td>6ff8bbc4856aa403bbd8990407c9c77a</td>\n      <td>0</td>\n      <td>Recommended by David Risher</td>\n      <td>Mon Jul 07 10:56:15 -0700 2014</td>\n      <td>Mon Jul 07 10:56:39 -0700 2014</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>151</td>\n      <td>daab5f2752243787e471e2ac01bf12fc</td>\n      <td>0</td>\n      <td>Well if Melanie says its her BBE, I gotta chec...</td>\n      <td>Mon May 14 12:55:56 -0700 2007</td>\n      <td>Sat Jan 07 11:40:38 -0800 2017</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>259028</td>\n      <td>fb4acc8a30bac6bf1414a03303d43c26</td>\n      <td>0</td>\n      <td>If steve recommends it, it must be good!</td>\n      <td>Thu Jan 18 11:09:48 -0800 2007</td>\n      <td>Mon Mar 09 00:38:30 -0700 2015</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>164051</th>\n      <td>50c82fffc560a19ca82ab87ee3a95b92</td>\n      <td>30258320</td>\n      <td>092f3faaf9db761193c6726a8fc82bf8</td>\n      <td>5</td>\n      <td>Alternative history/fantasy. Very interesting,...</td>\n      <td>Sat Apr 15 16:56:30 -0700 2017</td>\n      <td>Sat Apr 15 16:57:51 -0700 2017</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>164052</th>\n      <td>50c82fffc560a19ca82ab87ee3a95b92</td>\n      <td>18222716</td>\n      <td>4237dbb90067669edbc7978df8f5a31c</td>\n      <td>5</td>\n      <td>** spoiler alert ** \\n I greatly enjoyed this ...</td>\n      <td>Thu Jan 19 17:33:23 -0800 2017</td>\n      <td>Thu Jan 19 17:35:58 -0800 2017</td>\n      <td>Fri Jan 06 00:00:00 -0800 2017</td>\n      <td>Mon Jan 02 00:00:00 -0800 2017</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>164054</th>\n      <td>50c82fffc560a19ca82ab87ee3a95b92</td>\n      <td>22299763</td>\n      <td>7208191258727d84a429cd83a1dc4910</td>\n      <td>5</td>\n      <td>I dreamed this book--went to sleep and woke up...</td>\n      <td>Mon Dec 05 10:47:40 -0800 2016</td>\n      <td>Sat Apr 15 17:05:41 -0700 2017</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>164056</th>\n      <td>50c82fffc560a19ca82ab87ee3a95b92</td>\n      <td>2731276</td>\n      <td>0f2b6e84833089d7507bb611aa1053be</td>\n      <td>5</td>\n      <td>It's obvious that a great deal of care and tim...</td>\n      <td>Wed Nov 23 16:43:49 -0800 2016</td>\n      <td>Wed Nov 23 16:45:01 -0800 2016</td>\n      <td>Thu Sep 01 00:00:00 -0700 2016</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>164065</th>\n      <td>50c82fffc560a19ca82ab87ee3a95b92</td>\n      <td>17901125</td>\n      <td>ba1f7345a172e60dcc24591e8debec58</td>\n      <td>5</td>\n      <td>Love these characters and the setting. While t...</td>\n      <td>Fri Jul 10 20:04:58 -0700 2015</td>\n      <td>Fri Jul 10 20:09:36 -0700 2015</td>\n      <td>Fri Jul 10 00:00:00 -0700 2015</td>\n      <td>Tue Jul 07 00:00:00 -0700 2015</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>251704 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_train.rename({'rating': 'label'}, inplace=True, axis=1)\ntrain_ds = Dataset.from_pandas(new_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:56.916775Z","iopub.execute_input":"2023-01-27T17:07:56.917125Z","iopub.status.idle":"2023-01-27T17:07:57.593639Z","shell.execute_reply.started":"2023-01-27T17:07:56.917092Z","shell.execute_reply":"2023-01-27T17:07:57.592621Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"new_train['label'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:57.595432Z","iopub.execute_input":"2023-01-27T17:07:57.595829Z","iopub.status.idle":"2023-01-27T17:07:57.614316Z","shell.execute_reply.started":"2023-01-27T17:07:57.595789Z","shell.execute_reply":"2023-01-27T17:07:57.613489Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset, validation_dataset= train_ds.train_test_split(test_size=0.1).values()\ndata_all_splits = datasets.DatasetDict({\"train\":train_dataset, \"val\":validation_dataset})","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:57.619457Z","iopub.execute_input":"2023-01-27T17:07:57.619718Z","iopub.status.idle":"2023-01-27T17:07:57.681157Z","shell.execute_reply.started":"2023-01-27T17:07:57.619694Z","shell.execute_reply":"2023-01-27T17:07:57.680245Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_all_splits","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:57.682499Z","iopub.execute_input":"2023-01-27T17:07:57.683571Z","iopub.status.idle":"2023-01-27T17:07:57.693348Z","shell.execute_reply.started":"2023-01-27T17:07:57.683531Z","shell.execute_reply":"2023-01-27T17:07:57.692198Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['user_id', 'book_id', 'review_id', 'label', 'review_text', 'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes', 'n_comments', '__index_level_0__'],\n        num_rows: 226533\n    })\n    val: Dataset({\n        features: ['user_id', 'book_id', 'review_id', 'label', 'review_text', 'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes', 'n_comments', '__index_level_0__'],\n        num_rows: 25171\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# load the pre-trained model and its tokenizer","metadata":{}},{"cell_type":"code","source":"model_id = 'bert-base-uncased'\nmodel = BertForSequenceClassification.from_pretrained(model_id,num_labels=6)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:07:57.694636Z","iopub.execute_input":"2023-01-27T17:07:57.698297Z","iopub.status.idle":"2023-01-27T17:08:15.857889Z","shell.execute_reply.started":"2023-01-27T17:07:57.698269Z","shell.execute_reply":"2023-01-27T17:08:15.856841Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"befe8c8820a14c8fa127fdd5f40d9608"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d01e1071e2a4d91983cd4527e05b472"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:08:15.859552Z","iopub.execute_input":"2023-01-27T17:08:15.859923Z","iopub.status.idle":"2023-01-27T17:08:20.953305Z","shell.execute_reply.started":"2023-01-27T17:08:15.859886Z","shell.execute_reply":"2023-01-27T17:08:20.952296Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=6, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:08:20.954673Z","iopub.execute_input":"2023-01-27T17:08:20.955675Z","iopub.status.idle":"2023-01-27T17:08:23.010190Z","shell.execute_reply.started":"2023-01-27T17:08:20.955635Z","shell.execute_reply":"2023-01-27T17:08:23.009196Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e2016e86c1e441fb4330966883a93bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29244a66d1f64d778797b6b069e0c5e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e19d4cb1a984b888af2a5367a6009ba"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess(data):\n    return tokenizer(data['review_text'], padding=True, truncation=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:08:23.011443Z","iopub.execute_input":"2023-01-27T17:08:23.011792Z","iopub.status.idle":"2023-01-27T17:08:23.018390Z","shell.execute_reply.started":"2023-01-27T17:08:23.011758Z","shell.execute_reply":"2023-01-27T17:08:23.017366Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = data_all_splits.map(preprocess, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:08:23.020065Z","iopub.execute_input":"2023-01-27T17:08:23.020759Z","iopub.status.idle":"2023-01-27T17:11:40.041413Z","shell.execute_reply.started":"2023-01-27T17:08:23.020724Z","shell.execute_reply":"2023-01-27T17:11:40.039276Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/227 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00f7ec49f3c4aed8f58da65c900f779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0402dc2d5d84cebba3d215b6594e369"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:11:40.045704Z","iopub.execute_input":"2023-01-27T17:11:40.048036Z","iopub.status.idle":"2023-01-27T17:11:40.059711Z","shell.execute_reply.started":"2023-01-27T17:11:40.047995Z","shell.execute_reply":"2023-01-27T17:11:40.058520Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['user_id', 'book_id', 'review_id', 'label', 'review_text', 'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes', 'n_comments', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 226533\n    })\n    val: Dataset({\n        features: ['user_id', 'book_id', 'review_id', 'label', 'review_text', 'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes', 'n_comments', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 25171\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:11:40.064624Z","iopub.execute_input":"2023-01-27T17:11:40.067559Z","iopub.status.idle":"2023-01-27T17:11:40.075397Z","shell.execute_reply.started":"2023-01-27T17:11:40.067523Z","shell.execute_reply":"2023-01-27T17:11:40.074379Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:11:40.091587Z","iopub.execute_input":"2023-01-27T17:11:40.092978Z","iopub.status.idle":"2023-01-27T17:11:40.100554Z","shell.execute_reply.started":"2023-01-27T17:11:40.092944Z","shell.execute_reply":"2023-01-27T17:11:40.099575Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Connect notebook to hugging face account","metadata":{}},{"cell_type":"code","source":"!apt install git-lfs","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:11:40.102368Z","iopub.execute_input":"2023-01-27T17:11:40.103111Z","iopub.status.idle":"2023-01-27T17:11:43.088172Z","shell.execute_reply.started":"2023-01-27T17:11:40.103079Z","shell.execute_reply":"2023-01-27T17:11:43.086720Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.9.2-1).\n0 upgraded, 0 newly installed, 0 to remove and 115 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:11:43.090519Z","iopub.execute_input":"2023-01-27T17:11:43.090958Z","iopub.status.idle":"2023-01-27T17:11:43.135255Z","shell.execute_reply.started":"2023-01-27T17:11:43.090915Z","shell.execute_reply":"2023-01-27T17:11:43.134327Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f94c2ff7aa341259469fc02d854a6dd"}},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:13:06.405472Z","iopub.execute_input":"2023-01-27T17:13:06.406006Z","iopub.status.idle":"2023-01-27T17:13:06.672280Z","shell.execute_reply.started":"2023-01-27T17:13:06.405967Z","shell.execute_reply":"2023-01-27T17:13:06.671022Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# load metric","metadata":{}},{"cell_type":"code","source":"from datasets import load_metric\nf1_score_metric = load_metric('f1')\naccuracy_metric= load_metric(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    f1_score = f1_score_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    return {\"f1\": f1_score, \"accuracy\": accuracy}\n","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:13:06.674720Z","iopub.execute_input":"2023-01-27T17:13:06.675781Z","iopub.status.idle":"2023-01-27T17:13:07.456528Z","shell.execute_reply.started":"2023-01-27T17:13:06.675737Z","shell.execute_reply":"2023-01-27T17:13:07.455472Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d683b715b44288b5981749294a1700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8788cf4819784242a4c8397bfafa4a6f"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine tuning","metadata":{}},{"cell_type":"code","source":"batch_size = 16\nepochs = 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warmup_steps = 500\nweight_decay = 0.01","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir='Goodreads_Books_Reviews_BERT_51'\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    warmup_steps=warmup_steps,\n    weight_decay=weight_decay,\n    logging_dir='./logs',\n    push_to_hub=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\"\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:13:07.458410Z","iopub.execute_input":"2023-01-27T17:13:07.459447Z","iopub.status.idle":"2023-01-27T17:13:07.473769Z","shell.execute_reply.started":"2023-01-27T17:13:07.459405Z","shell.execute_reply":"2023-01-27T17:13:07.472670Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"val\"],\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:13:07.822630Z","iopub.execute_input":"2023-01-27T17:13:07.823927Z","iopub.status.idle":"2023-01-27T17:13:14.597702Z","shell.execute_reply.started":"2023-01-27T17:13:07.823876Z","shell.execute_reply":"2023-01-27T17:13:14.596267Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/huggingface_hub/repository.py:708: FutureWarning: Creating a repository through 'clone_from' is deprecated and will be removed in v0.11.\n  FutureWarning,\nCloning https://huggingface.co/lilouuch/Goodreads_Books_Reviews_BERT_51 into local empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-01-27T17:13:14.600651Z","iopub.execute_input":"2023-01-27T17:13:14.601081Z","iopub.status.idle":"2023-01-28T00:11:16.815726Z","shell.execute_reply.started":"2023-01-27T17:13:14.601038Z","shell.execute_reply":"2023-01-28T00:11:16.814295Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id. If date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 226533\n  Num Epochs = 2\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 14160\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14160' max='14160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14160/14160 6:57:55, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.947400</td>\n      <td>0.941511</td>\n      <td>0.616549</td>\n      <td>0.617894</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.829500</td>\n      <td>0.907856</td>\n      <td>0.636594</td>\n      <td>0.635533</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id. If date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 25171\n  Batch size = 32\nSaving model checkpoint to Goodreads_Books_Reviews_BERT_51/checkpoint-7080\nConfiguration saved in Goodreads_Books_Reviews_BERT_51/checkpoint-7080/config.json\nModel weights saved in Goodreads_Books_Reviews_BERT_51/checkpoint-7080/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThe following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id. If date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 25171\n  Batch size = 32\nSaving model checkpoint to Goodreads_Books_Reviews_BERT_51/checkpoint-14160\nConfiguration saved in Goodreads_Books_Reviews_BERT_51/checkpoint-14160/config.json\nModel weights saved in Goodreads_Books_Reviews_BERT_51/checkpoint-14160/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from Goodreads_Books_Reviews_BERT_51/checkpoint-14160 (score: 0.636594234520338).\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=14160, training_loss=0.9454089148569915, metrics={'train_runtime': 25082.1595, 'train_samples_per_second': 18.063, 'train_steps_per_second': 0.565, 'total_flos': 1.1921095464221491e+17, 'train_loss': 0.9454089148569915, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-01-28T00:11:16.818944Z","iopub.execute_input":"2023-01-28T00:11:16.820075Z","iopub.status.idle":"2023-01-28T00:19:56.948101Z","shell.execute_reply.started":"2023-01-28T00:11:16.820040Z","shell.execute_reply":"2023-01-28T00:19:56.947168Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id. If date_added, started_at, read_at, review_text, date_updated, review_id, user_id, n_votes, n_comments, __index_level_0__, book_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 25171\n  Batch size = 32\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='787' max='787' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [787/787 08:39]\n    </div>\n    "},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.9078561067581177,\n 'eval_f1': 0.636594234520338,\n 'eval_accuracy': 0.6355329545906003,\n 'eval_runtime': 520.1101,\n 'eval_samples_per_second': 48.396,\n 'eval_steps_per_second': 1.513,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()\n","metadata":{"execution":{"iopub.status.busy":"2023-01-28T00:19:56.950457Z","iopub.execute_input":"2023-01-28T00:19:56.950807Z","iopub.status.idle":"2023-01-28T00:20:12.586209Z","shell.execute_reply.started":"2023-01-28T00:19:56.950774Z","shell.execute_reply":"2023-01-28T00:20:12.584868Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Saving model checkpoint to Goodreads_Books_Reviews_BERT_51\nConfiguration saved in Goodreads_Books_Reviews_BERT_51/config.json\nModel weights saved in Goodreads_Books_Reviews_BERT_51/pytorch_model.bin\nDropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.636594234520338}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6355329545906003}]}\nTo https://huggingface.co/lilouuch/Goodreads_Books_Reviews_BERT_51\n   5f9cee5..ccffc0f  main -> main\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}